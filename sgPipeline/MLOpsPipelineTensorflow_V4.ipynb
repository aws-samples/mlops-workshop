{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 : Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import sagemaker.session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "mlops_bucket_ = \"mlops-{}-{}\".format(region, account_id) #+ region + \"-\" + account_id\n",
    "data_bucket_ = \"data-{}-{}\".format(region, account_id)\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=mlops_bucket_)\n",
    "pipeline_session = PipelineSession(default_bucket=mlops_bucket_)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "model_package_group_name = f\"AbalonePackageGroup\"\n",
    "\n",
    "print(\"AWS Region:\", region)\n",
    "print(\"AWS Account ID:\", account_id)\n",
    "print(\"mlops_bucket\", mlops_bucket_)\n",
    "print(\"data_bucket\", data_bucket_)\n",
    "print(\"Role\", role)\n",
    "print(\"model_package_group_name\", model_package_group_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Upload the Dataset to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the raw data is available \n",
    "#raw data for the lab is available in \"s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv\"\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data -O /root/mlops-workshop/data/abalone.csv\n",
    "\n",
    "# Create directory for storing data file\n",
    "!mkdir -p data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data -O data/abalone.csv\n",
    "\n",
    "# Download the data to local\n",
    "# !aws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv /root/mlops-workshop/data/abalone.csv\n",
    "\n",
    "!ls -ltr data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the raw data to data bucket.\n",
    "\n",
    "local_path = \"data/abalone.csv\"\n",
    "data_prefix = \"input/raw\"\n",
    "\n",
    "base_uri = f\"s3://{data_bucket_}/{data_prefix}\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path, \n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (ParameterInteger,ParameterString,ParameterFloat)\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "#Define Instance count and type paramters for Processing Step\n",
    "processing_instance_type=\"ml.m5.xlarge\"\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "\n",
    "#Define Instance type parameter for Training Step\n",
    "training_instance_type=\"ml.m5.xlarge\"\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "\n",
    "#Image Repository\n",
    "image_uri=\"{}.dkr.ecr.{}.amazonaws.com/abalone:latest\".format(account_id, region)\n",
    "\n",
    "#pipeline_execution_id=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    "#pipeline_name=ExecutionVariables.PIPELINE_NAME\n",
    "\n",
    "#Define Model Approval Status Parameter\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"Approved\",  # ModelApprovalStatus can be set to a default of \"PendingManualApproval\" if you don't want default approval.\n",
    "    )\n",
    "#model_approval_status = \"Approved\"\n",
    "\n",
    "# Define Data Input Parameter for Training\n",
    "input_data = input_data_uri\n",
    "\n",
    "mlops_bucket = ParameterString(name=\"mlops_bucket\", default_value=mlops_bucket_)\n",
    "data_bucket = ParameterString(name=\"data_bucket\", default_value=data_bucket_)\n",
    "\n",
    "# Define MSE Threshold Parameter for Model Evaluation\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define a Processing Step for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify if the directory for the ETL processing is created\n",
    "#!ls -ltr mlops-workshop/etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /root/mlops-workshop/sgPipeline/abalone/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Because this is a headerless CSV file, specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/abalone.csv\",\n",
    "        header=None, \n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    \n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}/train/validate.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    role=role,\n",
    "    #sagemaker_session=sagemaker_session,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"ETL\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=Join(\n",
    "                        on=\"/\",\n",
    "                        values=[\n",
    "                        \"s3:/\", data_bucket,\n",
    "                        \"input/raw/abalone.csv\",\n",
    "                            ],\n",
    "                        ), \n",
    "                      destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/train\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\", mlops_bucket,\n",
    "                    ExecutionVariables.PIPELINE_NAME,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"input/train\",\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/test\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\", mlops_bucket,\n",
    "                    ExecutionVariables.PIPELINE_NAME,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"input/test\",\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    code=\"/root/mlops-workshop/sgPipeline/abalone/preprocessing.py\",\n",
    ")\n",
    "\n",
    "print(\"step_process:\", step_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define a Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training step using Tensorflow framework\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': \"2000\", \n",
    "    'layers': \"2\", \n",
    "    'dense_layer': \"64\",\n",
    "    'batch_size': \"8\"\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=training_instance_type,\n",
    "    output_path=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\", mlops_bucket,\n",
    "                    ExecutionVariables.PIPELINE_NAME,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"output\",\n",
    "                ],\n",
    "    ),\n",
    "    #sagemaker_session=sagemaker.Session(),\n",
    "    sagemaker_session=pipeline_session,\n",
    "    hyperparameters=training_parameters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"training\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"step_train\", step_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Define a Processing Step for Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /root/mlops-workshop/sgPipeline/abalone/evaluation.py\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "prefix = '/opt/ml/processing/'\n",
    "# Sagemaker stores the dataset copied from S3\n",
    "input_path = os.path.join(prefix, 'input')\n",
    "# If something bad happens, write a failure file with the error messages and store here\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "evaluation_path =  os.path.join(output_path, 'evaluation')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "def load_model():\n",
    "    logger.info(\"Load Pre-Trained Model\")\n",
    "    model_path = os.path.join(input_path, \"model/model.tar.gz\")\n",
    "    with tarfile.open(model_path) as tar_file:\n",
    "        tar_file.extractall(\".\")\n",
    "    model = tf.keras.models.load_model(\"model.h5\")\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Evaluation mode ...\")\n",
    "    \n",
    "    try:\n",
    "        test_path = os.path.join(input_path, \"testing\")\n",
    "    \n",
    "        # Load 'h5' keras model\n",
    "        model = load_model()\n",
    "\n",
    "        # Specify the Column names in order to manipulate the specific columns for pre-processing\n",
    "        column_names = [\"rings\", \"length\", \"diameter\", \"height\", \"whole weight\", \n",
    "            \"shucked weight\", \"viscera weight\", \"shell weight\", \"sex_F\", \"sex_I\", \"sex_M\"]\n",
    "        \n",
    "        logger.info(\"Reading test data.\")\n",
    "        test_data = pd.read_csv(os.path.join(test_path, 'test.csv'), sep=',', names=column_names)\n",
    "        y_test = test_data['rings'].to_numpy()\n",
    "        x_test = test_data.drop(['rings'], axis=1).to_numpy()\n",
    "        x_test = preprocessing.normalize(x_test)\n",
    "        \n",
    "        #run predictions\n",
    "        predictions_ = model.predict(x_test)\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        mse = mean_squared_error(y_test, predictions_)\n",
    "        rmse = mean_squared_error(y_test, predictions_, squared=False)\n",
    "        std = np.std(np.array(y_test) - np.array(predictions_))\n",
    "        # Save Metrics to S3 for Model Package\n",
    "        logger.info(\"Root Mean Square Error: {}\".format(rmse))\n",
    "        logger.info(\"Mean Square Error: {}\".format(mse))\n",
    "        logger.info(\"Standard Deviation: {}\".format(std))\n",
    "        report_dict = {\n",
    "            \"regression_metrics\": {\n",
    "                'rmse': {\n",
    "                    'value': rmse\n",
    "                },\n",
    "                'mse': {\n",
    "                    'value': mse,\n",
    "                },\n",
    "                'standard_deviation': {\n",
    "                    'value': std,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        logger.info(\"Writing out evaluation report with mse: %f and std: %f\", mse, std)\n",
    "        if not os.path.exists(evaluation_path):\n",
    "            os.makedirs(evaluation_path)\n",
    "        evaluation_file = f\"{evaluation_path}/evaluation.json\"\n",
    "        with open(evaluation_file, \"w\") as f:\n",
    "            f.write(json.dumps(report_dict))\n",
    "        logger.info(\"Model evaluation completed.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # `DescribeTrainingJob` result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print('Exception during training: ' + str(e) + '\\\\n' + trc, file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "#    base_job_name=\"Evaluate\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"Evaluate\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/input/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/testing\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/output/evaluation\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\", mlops_bucket,\n",
    "                    ExecutionVariables.PIPELINE_NAME,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"evaluation\",\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    code=\"/root/mlops-workshop/sgPipeline/abalone/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "print(\"step_eval\", step_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Define a RegisterModel Step to Create a Model Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                    \"s3:/\", mlops_bucket,\n",
    "                    ExecutionVariables.PIPELINE_NAME,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    \"evaluation/evaluation.json\",\n",
    "            ],\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"Register\",\n",
    "    estimator=estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\", \"ml.c5.large\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "print(\"step_register\", step_register)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Define a Fail Step to Terminate the SageMaker Pipeline Exexution If Model Accuraacy is below required Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"Fail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE greater than\", mse_threshold]),\n",
    ")\n",
    "\n",
    "print(\"step_fail\", step_fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Define a Condition Step to Verify Model Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    ")\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\"\n",
    "    ),\n",
    "    right=6.0 #normally 6.0, 0.0 if we want to trigger the bad model detection.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(\n",
    "    name=\"Condition\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[step_fail], \n",
    ")\n",
    "\n",
    "print(\"step_cond\", step_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Finally Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"MLOps-SageMaker-Pipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        training_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        mse_threshold,\n",
    "        mlops_bucket,\n",
    "        data_bucket\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Pipeline Definition Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To View the pipeline\n",
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Pipeline Definition JSON into File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To View the pipeline\n",
    "import json\n",
    "#json.loads(pipeline.definition())\n",
    "mlops_pipeline = json.loads(pipeline.definition())\n",
    "\n",
    "# Writing to mlops-sm-pipeline.json\n",
    "with open(\"/root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(mlops_pipeline, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterize the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/-[0-9]\\{12,\\}/-<AccountId>/g' /root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json\n",
    "!sed -i 's/:[0-9]\\{12,\\}/:<AccountId>/g' /root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json\n",
    "!sed -i 's/'\"$AWS_ACCOUNT_ID\"'/<AccountId>/g' /root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json \n",
    "!sed -i 's/us-east-1/<Region>/g' /root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json \n",
    "!sed -i 's/\"\\RoleArn\\\"\\: \\\".*/\"\\RoleArn\\\"\\: \\\" \\\"\\,/' /root/mlops-workshop/sgPipeline/abalone/mlops_sm_pipeline_defn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Update the Pipeline\n",
    "\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the Execution of the Pipeline\n",
    "\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To examine a pipeline execution\n",
    "\n",
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
